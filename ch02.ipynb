{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b9f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "f = urlopen('http://hanbit.co.kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57da80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ff4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.getheader('Content-type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd52df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = f.info().get_content_charset(failobj='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cbf9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f.read().decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512bdf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ed2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from urllib.request import urlopen\n",
    "\n",
    "f = urlopen('http://www.hanbit.co.kr/store/books/full_book_list.html')\n",
    "f.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff496663",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = f.info().get_content_charset(failobj=\"utf-8\")\n",
    "print(\"encoding: \", encoding, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57323332",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f.read().decode(encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403596c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ff33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc98ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = urlopen('http://www.hanbit.co.kr/store/books/full_book_list.html')\n",
    "bytes_content = f.read()\n",
    "scanned_text = bytes_content[:1024].decode('ascii',errors='replace')\n",
    "print(scanned_text)\n",
    "match = re.search(r'charset=[\"\\']?([\\w-]+)', scanned_text)\n",
    "\n",
    "if match:\n",
    "    encoding = match.group(1)\n",
    "    print(\"encoding: \", encoding)\n",
    "else:\n",
    "    print('no match')\n",
    "\n",
    "print('encoding: ', encoding, file=sys.stderr)\n",
    "\n",
    "text = bytes_content.decode(encoding)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a38c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"abb abc akda\"\n",
    "re.search(r'a[]', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710a48f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645446c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'a.*c', 'abc123DEF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'a.*d','abc123DEF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(r'a.*d','abc123DEF',re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928098d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = re.search(r'a([A-Z])c([a-z])','123aFcs')\n",
    "m.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5934b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\w{2,3}', 'This is a pen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61078f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'\\w{2,3}', 'That', 'This is a pen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511330ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#urlopen_encoding.py\n",
    "import sys\n",
    "from urllib.request import urlopen\n",
    "f = urlopen('http://www.hanbit.co.kr/store/books/full_book_list.html')\n",
    "\n",
    "encoding = f.info().get_content_charset(failobj=\"utf-8\")\n",
    "print('encoding: ', encoding, file=sys.stderr)\n",
    "text = f.read().decode(encoding)\n",
    "print(text)\n",
    "hf = open(\"./dp.html\",'w',encoding='utf-8')\n",
    "\n",
    "for line in text:\n",
    "    hf.write(line)\n",
    "    \n",
    "hf.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-11 scrape_re.py 정규 표현식으로 스크레이핑 하기\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "\n",
    "with open('dp.html', encoding='utf-8') as f:\n",
    "    html = f.read()\n",
    "    \n",
    "#print(html)\n",
    "for partial_html in re.findall(r'<td class=\"left\"><a.*?</td>', html,re.DOTALL):\n",
    "    #print(partial_html)\n",
    "    url = re.search(r'<a href=\"(.*?)\">',partial_html).group(1)\n",
    "    #print(url)\n",
    "    url = 'http://www.hanbit.co.kr' + url\n",
    "    #http://www.hanbit.co.kr/store/books/look.php?p_code=B9483006177\n",
    "    title = re.sub(r'<.*?>','', partial_html)\n",
    "    title = unescape(title)\n",
    "    print('url: ', url)\n",
    "    print('title: ', title)\n",
    "    print('--'*20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0daa980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "\n",
    "tree = ElementTree.parse('rss.xml')\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "for item in root.findall('channel/item/description/body/location/data'):\n",
    "    \n",
    "    tm_ef = item.find('tmEf').text\n",
    "    tmn = item.find('tmn').text\n",
    "    tmx = item.find('tmx').text\n",
    "    wf = item.find('wf').text\n",
    "    print(tm_ef, tmn, tmx, wf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8cad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.15 save_csv.py\n",
    "import csv\n",
    "\n",
    "with open('top_cities.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['rank', 'city', 'population'])\n",
    "    writer.writerows([\n",
    "        [1,'상하이', 24150000],\n",
    "        [2,'카라치',23500000],\n",
    "        [3,'베이징',21516000],\n",
    "        [4,'텐진',14160467],\n",
    "        [5,'이스탄불',14160467],\n",
    "        \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.16 \n",
    "import csv\n",
    "\n",
    "with open('top_cities.csv','w', newline='') as f:\n",
    "    writer = csv.DictWriter(f,['rank','city','population'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows([\n",
    "        {'rank': 1, 'city': '상하이', 'population': 24150000},\n",
    "        {'rank': 2, 'city': '카라치', 'population': 23500000},\n",
    "        {'rank': 3, 'city': '베이징', 'population': 21516000},\n",
    "        {'rank': 4, 'city': '텐진',   'population': 14722100},\n",
    "        {'rank': 5, 'city': '아스탄불', 'population': 14160467},   \n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cities = [\n",
    "    {'rank': 1, 'city': '상하이', 'population': 24150000},\n",
    "    {'rank': 2, 'city': '카라치', 'population': 23500000},\n",
    "    {'rank': 3, 'city': '베이징', 'population': 21516000},\n",
    "    {'rank': 4, 'city': '텐진',   'population': 14722100},\n",
    "    {'rank': 5, 'city': '아스탄불', 'population': 14160467}, \n",
    "]\n",
    "print(json.dumps(cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(cities, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('top_cities.json', 'w') as f:\n",
    "    json.dump(cities,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 2.18 save_sqlite3.py\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('top_cities.db')\n",
    "\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('DROP TABLE IF EXISTS cities')\n",
    "\n",
    "c.execute('''\n",
    "    CREATE TABLE cities(\n",
    "        rank integer,\n",
    "        city text,\n",
    "        population integer\n",
    "    )\n",
    "''')\n",
    "\n",
    "c.execute('INSERT INTO cities VALUES (?,?,?)',(1,'상하이', 24150000))\n",
    "\n",
    "c.execute('INSERT INTO cities VALUES (:rank, :city, :population)',\n",
    "            {'rank':2,'city':'카라치', 'population': 23500000})\n",
    "\n",
    "c.executemany('INSERT INTO cities VALUES (:rank,:city,:population)',[\n",
    "            {'rank':3,'city':'베이징', 'population': 21516000},\n",
    "            {'rank':4,'city':'텐진',  'population': 14722100},\n",
    "            {'rank':5,'city':'이스탄불', 'population': 14160467},])\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "c.execute('SELECT * FROM cities')\n",
    "\n",
    "for row in c.fetchall():\n",
    "    print(row)\n",
    "    \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60edc3",
   "metadata": {},
   "source": [
    "### 2.7 파이썬으로 스크레이핑하는 흐름\n",
    "  * fetch(url)\n",
    "     - 매개변수로 url을 받고 지정한 URL의 웹 페이지를 추출한다.\n",
    "  * scrape(html)\n",
    "     - 매개변수로 html를 받고, 정규 표현식을 사용해 HTML에서 도서 정보추출\n",
    "  * save(db_path, books)\n",
    "     - 매개변수로 books라는 도서 목록을 받고, SQLite 데이터베이스에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "from urllib.request import urlopen\n",
    "from html import unescape\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    메인 처리입니다.\n",
    "    fetch(), scrap(), save()\n",
    "    \"\"\"\n",
    "    html = fetch('http://www.hanbit.co.kr/store/books/full_book_list.html')\n",
    "    #print(type(html))\n",
    "    books = scrape(html)\n",
    "    \n",
    "    save('books.db',books)\n",
    "    \n",
    "    \n",
    "def fetch(url):\n",
    "    f = urlopen(url)\n",
    "    encoding = f.info().get_content_charset(failobj='utf-8')\n",
    "    html = f.read().decode(encoding)\n",
    "    return html\n",
    "    \n",
    "def scrape(html):\n",
    "    \n",
    "    books = []\n",
    "    \n",
    "    for partial_html in re.findall(r'<td class=\"left\"><a.*?</td>', html, re.DOTALL):\n",
    "        url = re.search(r'<a href=\"(.*?)\">', partial_html).group(1)\n",
    "        url = 'http://www.hanbit.co.kr' + url\n",
    "        \n",
    "        title = re.sub(r'<.*?>','',partial_html)\n",
    "        title = unescape(title)\n",
    "        books.append({'url': url, 'title': title})\n",
    "        \n",
    "    \n",
    "    return books\n",
    "\n",
    "\n",
    "def save(db_path, books):\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    c = conn.cursor()\n",
    "    \n",
    "    c.execute('DROP TABLE IF EXISTS books')\n",
    "    \n",
    "    c.execute('''\n",
    "        CREATE TABLE books(\n",
    "            title text,\n",
    "            url   text\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    c.executemany('INSERT INTO books VALUES (:title, :url)', books)\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sqlite3 books.db \"SELECT * FROM books\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "from urllib.request import urlopen\n",
    "from html import unescape\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    메인 처리입니다.\n",
    "    fetch(), scrape(), save() 함수를 호출합니다.\n",
    "    \"\"\"\n",
    "    print('start main')\n",
    "    html = fetch('http://www.hanbit.co.kr/store/books/full_book_list.html')\n",
    "    books = scrape(html)\n",
    "    save('books.db', books)\n",
    "\n",
    "def fetch(url):\n",
    "    \"\"\"\n",
    "    매개변수로 전달받을 url을 기반으로 웹 페이지를 추출합니다.\n",
    "    웹 페이지의 인코딩 형식은 Content-Type 헤더를 기반으로 알아냅니다.\n",
    "    반환값: str 자료형의 HTML\n",
    "    \"\"\"\n",
    "    f = urlopen(url)\n",
    "    # HTTP 헤더를 기반으로 인코딩 형식을 추출합니다.\n",
    "    encoding = f.info().get_content_charset(failobj=\"utf-8\")\n",
    "    # 추출한 인코딩 형식을 기반으로 문자열을 디코딩합니다.\n",
    "    html = f.read().decode(encoding)\n",
    "    return html\n",
    "\n",
    "def scrape(html):\n",
    "    \"\"\"\n",
    "    매개변수 html로 받은 HTML을 기반으로 정규 표현식을 사용해 도서 정보를 추출합니다.\n",
    "    반환값: 도서(dict) 리스트\n",
    "    \"\"\"\n",
    "    books = []\n",
    "    # re.findall()을 사용해 도서 하나에 해당하는 HTML을 추출합니다.\n",
    "    for partial_html in re.findall(r'<td class=\"left\"><a.*?</td>', html, re.DOTALL):\n",
    "        # 도서의 URL을 추출합니다.\n",
    "        url = re.search(r'<a href=\"(.*?)\">', partial_html).group(1)\n",
    "        url = 'http://www.hanbit.co.kr' + url\n",
    "        # 태그를 제거해서 도서의 제목을 추출합니다.\n",
    "        title = re.sub(r'<.*?>', '', partial_html)\n",
    "        title = unescape(title)\n",
    "        books.append({'url': url, 'title': title})\n",
    "    \n",
    "    return books\n",
    "\n",
    "def save(db_path, books):\n",
    "    \"\"\"\n",
    "    매개변수 books로 전달된 도서 목록을 SQLite 데이터베이스에 저장합니다.\n",
    "    데이터베이스의 경로는 매개변수 dp_path로 지정합니다.\n",
    "    반환값: None(없음)\n",
    "    \"\"\"\n",
    "    # 데이터베이스를 열고 연결을 확립합니다.\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    # 커서를 추출합니다.\n",
    "    c = conn.cursor()\n",
    "    # execute() 메서드로 SQL을 실행합니다.\n",
    "    # 스크립트를 여러 번 실행할 수 있으므로 기존의 books 테이블을 제거합니다.\n",
    "    c.execute('DROP TABLE IF EXISTS books')\n",
    "    # books 테이블을 생성합니다.\n",
    "    c.execute('''\n",
    "        CREATE TABLE books (\n",
    "            title text,\n",
    "            url text\n",
    "        )\n",
    "    ''')\n",
    "    # executemany() 메서드를 사용하면 매개변수로 리스트를 지정할 수 있습니다.\n",
    "    c.executemany('INSERT INTO books VALUES (:title, :url)', books)\n",
    "    # 변경사항을 커밋(저장)합니다.\n",
    "    conn.commit()\n",
    "    # 연결을 종료합니다.\n",
    "    conn.close()\n",
    "\n",
    "# python 명령어로 실행한 경우 main() 함수를 호출합니다.\n",
    "# 이는 모듈로써 다른 파일에서 읽어 들였을 때 main() 함수가 호출되지 않게 하는 것입니다.\n",
    "# 파이썬 프로그램의 일반적인 작성 방식입니다.\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce106cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(\"http://hanbit.co.kr\")\n",
    "print(type(r))\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.headers['content-type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25270190",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb08fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e97517",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3563b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r =requests.get('https://jsonplaceholder.typicode.com/todos/1')\n",
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8422b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba194e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('http://httpbin.org/post', data={'key1':'value1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aebb6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46931b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = lxml.html.parse('full_book_list.html')\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = tree.getroot()\n",
    "\n",
    "for a in html.xpath('//a'):\n",
    "    print(a.get('href'),a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = lxml.html.fromstring('''\n",
    "<html>\n",
    "   <head><title>온라인 과일 가게</title></head>\n",
    "   <body>\n",
    "      <h1 id=\"main\" class=\"test\">오늘의 과일</h1>\n",
    "      <ul>\n",
    "          <li>사과</li>\n",
    "          <li class=\"featured\">귤</li>\n",
    "          <li>포도</li>\n",
    "     </ul>\n",
    "   </body>\n",
    "</html>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f10f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3a7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath('//li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb31b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in html.xpath('//li'):\n",
    "    print(a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = html.xpath('//h1')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07036a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea71e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52446904",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6129462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.getparent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e268d",
   "metadata": {},
   "source": [
    "## 웹 서버에 요청하고 응답받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93bf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://www.python.org/\"\n",
    "resp = requests.get(url)\n",
    "print(resp)\n",
    "\n",
    "url2 = \"https://www.python.org/1\"\n",
    "resp = requests.get(url2)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cade3a",
   "metadata": {},
   "source": [
    "## 웹 페이지 소스코드 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ddd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://www.python.org\"\n",
    "resp = requests.get(url)\n",
    "#print(resp.__dict__)\n",
    "print(resp.__dir__())\n",
    "print('--'*25)\n",
    "print(resp.headers)\n",
    "print('--'*25)\n",
    "print(resp.status_code)\n",
    "print('--'*25)\n",
    "#print(resp.content)\n",
    "print('--'*25)\n",
    "print(resp.url)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bba2dd",
   "metadata": {},
   "source": [
    "## 로봇 배제 표준(robots.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ed34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "urls = ['https://www.naver.com/','https://www.python.org/']\n",
    "filename = 'robots.txt'\n",
    "\n",
    "for url in urls:\n",
    "    file_path = url + filename\n",
    "    resp = requests.get(file_path)\n",
    "    print(file_path)\n",
    "    print(resp.text)\n",
    "    print('--'*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e45f5c",
   "metadata": {},
   "source": [
    "## BeautifulSoup 객체 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ae0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c40d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_doc,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6db69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc313385",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac932ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be091cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4293f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(id=\"link3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d12381",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup('<b class=\"boldest\">Extremely bold</b>')\n",
    "tag = soup.b\n",
    "tag['class']\n",
    "tag.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway'\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "#print(html_src)\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "print('title 태그 요소: ', soup.title)\n",
    "print('title 태그 이름: ', soup.title.name)\n",
    "print('title 태그 문자열: ', soup.title.string)\n",
    "first_img = soup.find(name='img',attrs={'alt':'Seoul-metro-2009-20180916-103548.jpg'})\n",
    "print(first_img)\n",
    "\n",
    "first_img = soup.select('#mw-content-text > div.mw-parser-output > table:nth-child(5) > tbody > tr:nth-child(2) > td > a > img')\n",
    "print(first_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acac2eb",
   "metadata": {},
   "source": [
    "## 웹 문서의 그림 이미지 파일을 PC에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway'\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "\n",
    "target_img = soup.find(name='img', attrs={'alt': 'Seoul-metro-2009-20180916-103548.jpg'})\n",
    "print('HTML 요소: ', target_img)\n",
    "print('--'*25)\n",
    "target_img_src = target_img.get('src')\n",
    "print('이미지 파일 경로: ',target_img_src)\n",
    "print('--'*25)\n",
    "target_img_resp = requests.get('http:' + target_img_src)\n",
    "print(target_img_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64352a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway\"\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "\n",
    "target_img = soup.find(name='img', attrs={'alt':'Seoul-metro-2009-20180916-103548.jpg'})\n",
    "#print('HTML 요소: ', target_img)\n",
    "#print('--'*25)\n",
    "target_img_src = target_img.get('src')\n",
    "\n",
    "\n",
    "\n",
    "#target_img_src = target_img.get('srcset')\n",
    "#print('이미지 파일 경로: ', target_img_src)\n",
    "#print(\"--\"*25)\n",
    "# print(type(target_img_src))\n",
    "# target_img_str = target_img_src.split(',')\n",
    "# print(target_img_str[0])\n",
    "# print(target_img_str[1])\n",
    "# target_img_str = target_img_str[1].split(' ')\n",
    "# print(target_img_str)\n",
    "# target_img_str = target_img_str[1]\n",
    "\n",
    "#target_img_resp = requests.get('http:'+ '//upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Seoul-metro-2009-20180916-103548.jpg/600px-Seoul-metro-2009-20180916-103548.jpg')\n",
    "target_img_resp = requests.get('http:'+ target_img_src+'1' )\n",
    "\n",
    "print(target_img_src)\n",
    "print(target_img_resp.status_code)\n",
    "lst = []\n",
    "if target_img_resp.status_code != 200:\n",
    "    target_img_src = target_img.get('srcset')\n",
    "    target_img_src = target_img_src.split(',')\n",
    "    #print(target_img_src)\n",
    "    #print(target_img_src[0])\n",
    "    src = target_img_src[0].split(' ')\n",
    "    #print(src)\n",
    "    lst.append(src[0])\n",
    "    \n",
    "    #print(target_img_src[1])\n",
    "    src = target_img_src[1].split()\n",
    "    #src = src[0]\n",
    "    print('src: ',src)\n",
    "    lst.append(src[0])\n",
    "    \n",
    "    #print('-'*50)\n",
    "    #print(\"lst: \",lst)\n",
    "    #print('-'*50)    \n",
    "    target_img_resp = requests.get('http:'+ lst[0] )\n",
    "    #print(lst[0])\n",
    "    #print(target_img_resp)\n",
    "    out_file_path = \"download_image1.jpg\"\n",
    "\n",
    "    with open(out_file_path, 'wb') as out_file:\n",
    "         out_file.write(target_img_resp.content)\n",
    "         print('이미지 파일로 저장하였습니다.')\n",
    "\n",
    "    #print('-'*50)\n",
    "\n",
    "    target_img_resp = requests.get('http:'+ lst[1] )\n",
    "    #print(lst[1])\n",
    "    #print(target_img_resp)\n",
    "\n",
    "    out_file_path = \"download_image2.jpg\"\n",
    "\n",
    "    with open(out_file_path, 'wb') as out_file:\n",
    "         out_file.write(target_img_resp.content)\n",
    "         print('이미지 파일로 저장하였습니다.')\n",
    "        \n",
    "else:\n",
    "    \n",
    "    out_file_path = \"download_image.jpg\"\n",
    "\n",
    "    with open(out_file_path, 'wb') as out_file:\n",
    "         out_file.write(target_img_resp.content)\n",
    "         print('이미지 파일로 저장하였습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549fe1f",
   "metadata": {},
   "source": [
    "## 웹 문서에 포함된 모든 하이퍼링크 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea045569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway\"\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "\n",
    "links = soup.find_all(\"a\")\n",
    "print(type(links))\n",
    "print(\"하이퍼링크의 개수: \", len(links))\n",
    "print('--'* 25)\n",
    "print('첫 3개의 원소: ', links[:3] )\n",
    "print('--'* 25)\n",
    "wiki_links = soup.find_all(name='a', href=re.compile(\"/wiki/\"), limit=3)\n",
    "print('/wiki/문자열이 포함된 하이퍼링크: ', wiki_links)\n",
    "print('--'* 25)\n",
    "external_links = soup.find_all(name='a', attrs={'class':\"external text\"}, limit=3)\n",
    "print(\"class 속성으로 추출한 하이퍼링크: \", external_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f5c77",
   "metadata": {},
   "source": [
    "## CSS Selector 활용하기 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8acc7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway\"\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "\n",
    "subway_image =soup.select('#mw-content-text > div.mw-parser-output > table:nth-child(5) > tbody > tr:nth-child(2) > td > a > img')\n",
    "# print(subway_image)\n",
    "# print(subway_image[0])\n",
    "\n",
    "subway_image2 = soup.select('a > img')\n",
    "print(len(subway_image2))\n",
    "\n",
    "for i in range(5):\n",
    "    print(subway_image2[i])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1bcd26",
   "metadata": {},
   "source": [
    "## CSS Selector 활용하기 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec269b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Seoul_Metropolitan_Subway\"\n",
    "resp = requests.get(url)\n",
    "html_src = resp.text\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "\n",
    "links = soup.select('a')\n",
    "print(len(links))\n",
    "print()\n",
    "print(links[:3])\n",
    "\n",
    "print()\n",
    "external_links = soup.select('a[class=\"external text\"]')\n",
    "print(external_links[:3])\n",
    "print()\n",
    "\n",
    "external_links = soup.select('a.external text')\n",
    "print(external_links[:3])\n",
    "print()\n",
    "\n",
    "jump_links = soup.select('a[class=\"mw-jump-link\"]')\n",
    "print(jump_links[:3])\n",
    "print()\n",
    "jump_links = soup.select('a.mw-jump-link')\n",
    "print(jump_links[:3])\n",
    "print()\n",
    "\n",
    "id_selector = soup.select('#siteNotice')\n",
    "print('id_selector: ',id_selector)\n",
    "id_selector2 = soup.select('div#siteNotice')\n",
    "print('id_selector2: ', id_selector2)\n",
    "\n",
    "class_selector = soup.select('.mw-headline')\n",
    "print('class_selector: ', class_selector)\n",
    "\n",
    "print('--'*30)\n",
    "class_selector = soup.select('span.mw-headline')\n",
    "print('class_selector: ', class_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419465ac",
   "metadata": {},
   "source": [
    "## 구글 뉴스 클리핑하기 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533f5f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://news.google.com\"\n",
    "search_url = base_url + \"/search?q=반도체&hl=ko&gl=KR&ceid=KR%3Ako\"\n",
    "\n",
    "resp = requests.get(search_url)\n",
    "html_src = resp.text\n",
    "soup = BeautifulSoup(html_src, 'html.parser')\n",
    "#print(soup)\n",
    "\n",
    "news_items = soup.select('div[class=\"xrnccd\"]')\n",
    "# print(len(news_items))\n",
    "# print(news_items[0])\n",
    "\n",
    "# for item in news_items:\n",
    "#     news_title = item.find('a',attrs={'class':'DY5T1d'}).getText()\n",
    "#     print(news_title)\n",
    "#     news_agency = item.find('a',attrs={'class': 'wEwyrc'}).text\n",
    "#     print(news_agency)\n",
    "#     news_reporting = item.find('time', attrs={'class':'WW6dff'}).text\n",
    "#     print(news_reporting)\n",
    "    \n",
    "#     news_reporting = item.find('time', attrs={'class':'WW6dff'})\n",
    "#     news_reporting_datetime = news_reporting.get('datetime').split('T')\n",
    "#     news_reporting_date = news_reporting_datetime[0]\n",
    "#     news_reporting_time = news_reporting_datetime[1][:-1]\n",
    "#     print(news_reporting_date, news_reporting_time)\n",
    "#     print()\n",
    "\n",
    "def google_news_clipping(url,limit=5):\n",
    "    resp=requests.get(url)\n",
    "    html_src = resp.text\n",
    "    soup = BeautifulSoup(html_src,'html.parser')\n",
    "    \n",
    "    news_items = soup.select('div[class=\"xrnccd\"]')\n",
    "    \n",
    "    links = []; titles=[]; agencies=[]; reporting_dates=[]; reporting_times=[];\n",
    "    \n",
    "    for item in news_items[:limit]:\n",
    "        link = item.find('a', attrs={'class':'VDXfz'}).get('href')\n",
    "        news_link = base_url + link[1:]\n",
    "        links.append(news_link)\n",
    "        \n",
    "        news_title = item.find('a',attrs={'class':'DY5T1d'}).getText()\n",
    "        titles.append(news_title)\n",
    "        \n",
    "        news_agency = item.find('a',attrs={'class': 'wEwyrc'}).text\n",
    "        agencies.append(news_agency)\n",
    "        \n",
    "        news_reporting = item.find('time', attrs={'class':'WW6dff'})\n",
    "        news_reporting_datetime = news_reporting.get('datetime').split('T')\n",
    "                                   \n",
    "        news_reporting_date = news_reporting_datetime[0]\n",
    "        news_reporting_time = news_reporting_datetime[1][:-1]\n",
    "        reporting_dates.append(news_reporting_date)\n",
    "        reporting_times.append(news_reporting_time)\n",
    "    \n",
    "    result = {'link': links, 'title': titles, 'agency': agencies, \\\n",
    "              'date': reporting_dates, 'time': reporting_times}\n",
    "        \n",
    "    return result\n",
    "\n",
    "def display(news,num):\n",
    "    \n",
    "    for i in range(num):\n",
    "        print(news['title'][i])\n",
    "        print(news['agency'][i])\n",
    "        print(news['date'][i])\n",
    "        print(news['time'][i])\n",
    "        print(news['link'][i])\n",
    "        print()\n",
    "        print()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "news = google_news_clipping(search_url, 5)\n",
    "\n",
    "display(news,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4367ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "# keyword_input = '아파트'\n",
    "# keyword = urllib.parse.quote(keyword_input)\n",
    "# print('파이썬 문자열을 URL 코드로 변환: ', keyword)\n",
    "\n",
    "base_url = \"https://news.google.com\"\n",
    "\n",
    "def google_news_clipping(keyword_input,limit=5):\n",
    "    \n",
    "    \n",
    "    keyword = urllib.parse.quote(keyword_input)\n",
    "    \n",
    "    url = base_url + \"/search?q=\" + keyword + \"&hl=ko&gl=KR&ceid=KR%3Ako\"\n",
    "    resp=requests.get(url)\n",
    "    html_src = resp.text\n",
    "    soup = BeautifulSoup(html_src,'html.parser')\n",
    "    \n",
    "    news_items = soup.select('div[class=\"xrnccd\"]')\n",
    "    \n",
    "    links = []; titles=[]; agencies=[]; reporting_dates=[]; reporting_times=[];\n",
    "    \n",
    "    for item in news_items[:limit]:\n",
    "        link = item.find('a', attrs={'class':'VDXfz'}).get('href')\n",
    "        news_link = base_url + link[1:]\n",
    "        links.append(news_link)\n",
    "        \n",
    "        news_title = item.find('a',attrs={'class':'DY5T1d'}).getText()\n",
    "        titles.append(news_title)\n",
    "        \n",
    "        news_agency = item.find('a',attrs={'class': 'wEwyrc'}).text\n",
    "        agencies.append(news_agency)\n",
    "        \n",
    "        news_reporting = item.find('time', attrs={'class':'WW6dff'})\n",
    "        news_reporting_datetime = news_reporting.get('datetime').split('T')\n",
    "                                   \n",
    "        news_reporting_date = news_reporting_datetime[0]\n",
    "        news_reporting_time = news_reporting_datetime[1][:-1]\n",
    "        reporting_dates.append(news_reporting_date)\n",
    "        reporting_times.append(news_reporting_time)\n",
    "    \n",
    "    result = {'link': links, 'title': titles, 'agency': agencies, \\\n",
    "              'date': reporting_dates, 'time': reporting_times}\n",
    "        \n",
    "    return result\n",
    "\n",
    "def display(news,num):\n",
    "    \n",
    "    for i in range(num):\n",
    "        print(news['title'][i])\n",
    "        print(news['agency'][i])\n",
    "        print(news['date'][i])\n",
    "        print(news['time'][i])\n",
    "        print(news['link'][i])\n",
    "        print()\n",
    "        print()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "search_word = input(\"검색어를 입력하세요: \")\n",
    "#print(\"search_word: \", search_word)\n",
    "news = google_news_clipping(search_word, 5)\n",
    "\n",
    "display(news,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pw = \"1225danawa@\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8e798",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=chrome_options)\n",
    "driver.implicitly_wait(3)\n",
    "url = \"https://www.danawa.com\"\n",
    "#headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36\"}\n",
    "driver.get(url)\n",
    "\n",
    "login = driver.find_element_by_css_selector('#danawa_header > div > div > div.main-header__banner > div.main-header__user > div:nth-child(5) > a')\n",
    "\n",
    "#print('HTML 요소: ', login)\n",
    "#print('태그 이름: ', login.tag_name)\n",
    "#print('문자열: ', login.text)\n",
    "#print('href 속성: ', login.get_attribute('href'))\n",
    "\n",
    "driver.implicitly_wait(10)\n",
    "login.click()\n",
    "\n",
    "driver.implicitly_wait(10)\n",
    "#driver.close()\n",
    "\n",
    "my_id = \"skypwk\"\n",
    "\n",
    "\n",
    "driver.find_element_by_id('danawa-member-login-input-id').send_keys(my_id)\n",
    "driver.implicitly_wait(3)\n",
    "driver.find_element_by_name('password').send_keys(my_pw)\n",
    "driver.implicitly_wait(3)\n",
    "driver.find_element_by_css_selector('button.btn_login').click()\n",
    "\n",
    "wishlist = driver.find_element_by_css_selector('#danawa_header > div > div > div.main-header__banner > div.main-header__user > div:nth-child(4) > a')\n",
    "wishlist.click()\n",
    "driver.implicitly_wait(2)\n",
    "html_src = driver.page_source\n",
    "#print(html_src)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "soup = BeautifulSoup(html_src,'html.parser')\n",
    "\n",
    "wish_table = soup.select('table[class=\"tbl wish_tbl\"]')[0]\n",
    "#print(wish_table)\n",
    "#print(type(wish_table))\n",
    "wish_items = wish_table.select('tbody tr')\n",
    "#print(wish_items)\n",
    "\n",
    "for item in wish_items:\n",
    "    title = item.find('div',{'class': 'tit'}).text\n",
    "    price = item.find('span',{'class': 'price'}).text\n",
    "    link = item.find('a',href=re.compile('http://prod.danawa.com/info/')).get('href')\n",
    "    print('title: ',title)\n",
    "    print('price: ', price)\n",
    "    print('link: ', link)\n",
    "    print('--'* 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3cf19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "driver.implicitly_wait(30)\n",
    "driver.get(\"https://ecos.bok.or.kr/jsp/vis/keystat/#/key\")\n",
    "\n",
    "excel_download = driver.find_element_by_css_selector('img[alt=\"download\"]')\n",
    "driver.implicitly_wait(3)\n",
    "excel_download.click()\n",
    "time.sleep(5)\n",
    "print(\"파일 다운로드 실행...\")\n",
    "\n",
    "\n",
    "#driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab75b2c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    "def download_bok_statistics_by_keyword():\n",
    "    keyword=\"\"\n",
    "\n",
    "    keyword = str(input(\"검색할 항목을 입력하세요: \"))\n",
    "\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver.implicitly_wait(3)\n",
    "    driver.get(\"https://ecos.bok.or.kr/jsp/vis/keystat/#/key\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    items1 = driver.find_elements_by_css_selector('a[class=\"ng-binding\"]')\n",
    "    items2 = driver.find_elements_by_css_selector('a[class=\"a-c1-list ng-binding\"]')\n",
    "    items3 = driver.find_elements_by_css_selector('a[class=\"a-c4-list ng-binding\"]')\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    items = items1[1:] + items2 + items3\n",
    "\n",
    "    for item in items:\n",
    "        if keyword in item.text:\n",
    "            print(\"검색어 '%s'에 매칭 되는 '%s' 통계지표를 검색중...\" % (keyword, item.text))\n",
    "            item.click()\n",
    "            time.sleep(2)\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    html_src = driver.page_source\n",
    "    soup = BeautifulSoup(html_src, 'html.parser')\n",
    "    driver.close()\n",
    "    table_items = soup.find_all('td',{'class':'ng-binding'})\n",
    "    date = [t.text for i, t in enumerate(table_items) if i % 3 == 0]\n",
    "    value = [t.text for i, t in enumerate(table_items) if i % 3 == 1]\n",
    "    change = [t.text for i, t in enumerate(table_items) if i % 3 == 2]\n",
    "\n",
    "    with open('bok_statistics_%s.csv' % keyword,'w') as result_file:\n",
    "        for i in range(len(date)):\n",
    "            result_file.write(\"%s, %s, %s\" % (date[i], value[i], change[i]))\n",
    "            result_file.write('\\n')    \n",
    "    return date,value,change\n",
    "        \n",
    "result = download_bok_statistics_by_keyword()\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "def download_bok_statistics_by_keyword():\n",
    "    \n",
    "    item_found = 0              \n",
    "    while not item_found:\n",
    "        \n",
    "        keyword = \"\"       \n",
    "        while len(keyword) == 0:\n",
    "            keyword = str(input(\"검색할 항목을 입력하세요: \"))\n",
    "        \n",
    "        #driver = webdriver.Chrome(\"chromedriver\")\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        driver.implicitly_wait(3)\n",
    "        driver.get(\"http://ecos.bok.or.kr/jsp/vis/keystat/#/key\") \n",
    "        time.sleep(5)             \n",
    "                   \n",
    "        items1 = driver.find_elements_by_css_selector('a[class=\"ng-binding\"]')\n",
    "        items2 = driver.find_elements_by_css_selector('a[class=\"a-c1-list ng-binding\"]')\n",
    "        items3 = driver.find_elements_by_css_selector('a[class=\"a-c4-list ng-binding\"]')\n",
    "        driver.implicitly_wait(3)\n",
    "\n",
    "        items = items1[1:] + items2 + items3    \n",
    "        \n",
    "        for idx, item in enumerate(items):\n",
    "            if keyword in item.text:\n",
    "                print(\"검색어 '%s'에 매칭되는 '%s' 통계지표를 검색 중...\" % (keyword, item.text))\n",
    "                item.click()\n",
    "                item_found = 1\n",
    "                time.sleep(5)\n",
    "                break\n",
    "            elif idx == (len(items) - 1):\n",
    "                print(\"검색어 '%s'에 대한 통계지표가 존재하지 않습니다...\" % keyword)\n",
    "                driver.close()\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    html_src = driver.page_source\n",
    "    soup = BeautifulSoup(html_src, 'html.parser')\n",
    "    driver.close()\n",
    "  \n",
    "    table_items = soup.find_all('td', {'class':'ng-binding'})\n",
    "    date = [t.text for i, t in enumerate(table_items) if i % 3 == 0]\n",
    "    value = [t.text for i, t in enumerate(table_items) if i % 3 == 1]    \n",
    "    change = [t.text for i, t in enumerate(table_items) if i % 3 == 2]    \n",
    "    \n",
    "    for i in range(len(date)):\n",
    "        print(\"%s, %s, %s\" % (date[i], value[i], change[i]))\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    result_file = open('bok_statistics_%s.csv' % keyword, 'w')\n",
    "    \n",
    "    for i in range(len(date)):\n",
    "        result_file.write(\"%s, %s, %s\" % (date[i], value[i], change[i]))\n",
    "        result_file.write('\\n')\n",
    "    \n",
    "    result_file.close()\n",
    "    print(\"키워드 '%s'에 대한 통계지표를 저장하였습니다.\" % keyword)\n",
    "    \n",
    "    return date, value, change\n",
    "\n",
    "result = download_bok_statistics_by_keyword()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db882b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = ['a','b','c','d','e','f','g','h','i']\n",
    "\n",
    "\n",
    "# for cnt,val in enumerate(lst):\n",
    "#     print(cnt,\":\",val)\n",
    "\n",
    "[ t for i, t in enumerate(lst) if i % 3 == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i,t in enumerate(lst):\n",
    "    if i % 3 == 0:\n",
    "        test.append(t)\n",
    "        \n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43956430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://comic.naver.com/webtoon/weekday\"\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(res.text,\"lxml\")\n",
    "\n",
    "cartoons = soup.find_all(\"a\",attrs={\"class\":\"title\"})\n",
    "\n",
    "#print(len(cartoons))\n",
    "for cartoon in cartoons:\n",
    "    print(cartoon.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "603c0848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연애혁명-397. 주객전도\n",
      "독립일기-시즌2 62화 스티커 빵\n",
      "더 복서-더 복서 외전. 악연(10)\n",
      "--------------------------------------------------------------------------------\n",
      "연애혁명-397. 주객전도\n",
      "독립일기-시즌2 62화 스티커 빵\n",
      "더 복서-더 복서 외전. 악연(10)\n",
      "--------------------------------------------------------------------------------\n",
      "연애혁명-397. 주객전도\n",
      "독립일기-시즌2 62화 스티커 빵\n",
      "더 복서-더 복서 외전. 악연(10)\n",
      "--------------------------------------------------------------------------------\n",
      "find:  연애혁명-397. 주객전도\n",
      "find:  독립일기-시즌2 62화 스티커 빵\n",
      "find:  더 복서-더 복서 외전. 악연(10)\n",
      "독립일기-시즌2 62화 스티커 빵\n",
      "더 복서-더 복서 외전. 악연(10)\n",
      "현실퀘스트-24화\n",
      "정글쥬스-66화\n",
      "나노마신-095. 제36장. 뱀의 아가리 속 (5)\n",
      "이두나!-000! (1)\n",
      "기기괴괴-373화 겨울나무 #4\n",
      "별을 삼킨 너에게-91화\n",
      "오빠세끼-#36 숨바꼭질\n",
      "--------------------------------------------------------------------------------\n",
      "rank1.parent:  <ol class=\"asideBoxRank\" id=\"realTimeRankFavorite\">\n",
      "<li class=\"rank01\">\n",
      "<a href=\"/webtoon/detail?titleId=570503&amp;no=401\" onclick=\"nclk_v2(event,'rnk*p.cont','570503','1')\" title=\"연애혁명-397. 주객전도\">연애혁명-397. 주객전도</a>\n",
      "<span class=\"rankBox\">\n",
      "<img alt=\"변동없음\" height=\"10\" src=\"https://ssl.pstatic.net/static/comic/images/migration/common/arrow_no.gif\" title=\"변동없음\" width=\"7\"/> 0\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</span>\n",
      "</li>\n",
      "<li class=\"rank02\">\n",
      "<a href=\"/webtoon/detail?titleId=748105&amp;no=165\" onclick=\"nclk_v2(event,'rnk*p.cont','748105','2')\" title=\"독립일기-시즌2 62화 스티커 빵\">독립일기-시즌2 62화 스티커 빵</a>\n",
      "<span class=\"rankBox\">\n",
      "<img alt=\"변동없음\" height=\"10\" src=\"https://ssl.pstatic.net/static/comic/images/migration/common/arrow_no.gif\" title=\"변동없음\" width=\"7\"/> 0\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</span>\n",
      "</li>\n",
      "<li class=\"rank03\">\n",
      "<a href=\"/webtoon/detail?titleId=736989&amp;no=124\" onclick=\"nclk_v2(event,'rnk*p.cont','736989','3')\" title=\"더 복서-더 복서 외전. 악연(10)\">더 복서-더 복서 외전. 악연(10)</a>\n",
      "<span class=\"rankBox\">\n",
      "<img alt=\"변동없음\" height=\"10\" src=\"https://ssl.pstatic.net/static/comic/images/migration/common/arrow_no.gif\" title=\"변동없음\" width=\"7\"/> 0\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</span>\n",
      "</li>\n",
      "<li class=\"rank04\">\n",
      "<a href=\"/webtoon/detail?titleId=783888&amp;no=24\" onclick=\"nclk_v2(event,'rnk*p.cont','783888','4')\" title=\"현실퀘스트-24화\">현실퀘스트-24화</a>\n",
      "<span class=\"rankBox\">\n",
      "<img alt=\"변동없음\" height=\"10\" src=\"https://ssl.pstatic.net/static/comic/images/migration/common/arrow_no.gif\" title=\"변동없음\" width=\"7\"/> 0\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</span>\n",
      "</li>\n",
      "<li class=\"rank05\">\n",
      "<a href=\"/webtoon/detail?titleId=762279&amp;no=66\" onclick=\"nclk_v2(event,'rnk*p.cont','762279','5')\" title=\"정글쥬스-66화\">정글쥬스-66화</a>\n",
      "<span class=\"rankBox\">\n",
      "<img alt=\"순위상승\" height=\"10\" src=\"https://ssl.pstatic.net/static/comic/images/migration/common/arrow_up.gif\" title=\"순위상승\" width=\"7\"/>1\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</span>\n",
      "</li>\n",
      "<li class=\"rank06\">\n",
      "<a href=\"/webtoon/detail?titleId=747271&amp;no=95\" onclick=\"nclk_v2(event,'rnk*p.cont','747271','6')\" title=\"나노마신-095. 제36장. 뱀의 아가리 속 (5)\">나노마신-095. 제36장. 뱀의 아가리 속 (5)</a>\n",
      "<span class=\"rankBox\">\n",
      "<img alt=\"순위하락\" height=\"10\" src=\"https://ssl.pstatic.net/static/comic/images/migration/common/arrow_down.gif\" title=\"순위하락\" width=\"7\"/>1\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</span>\n",
      "</li>\n",
      "<li class=\"rank07\">\n",
      "<a href=\"/webtoon/detail?titleId=731130&amp;no=141\" onclick=\"nclk_v2(event,'rnk*p.cont','731130','7')\" title=\"이두나!-000! (1)\">이두나!-000! (1)</a>\n",
      "<span class=\"rankBox\">\n",
      "<img alt=\"변동없음\" height=\"10\" src=\"https://ssl.pstatic.net/static/comic/images/migration/common/arrow_no.gif\" title=\"변동없음\" width=\"7\"/> 0\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</span>\n",
      "</li>\n",
      "<li class=\"rank08\">\n",
      "<a href=\"/webtoon/detail?titleId=557672&amp;no=395\" onclick=\"nclk_v2(event,'rnk*p.cont','557672','8')\" title=\"기기괴괴-373화 겨울나무 #4\">기기괴괴-373화 겨울나무 #4</a>\n",
      "<span class=\"rankBox\">\n",
      "<img alt=\"변동없음\" height=\"10\" src=\"https://ssl.pstatic.net/static/comic/images/migration/common/arrow_no.gif\" title=\"변동없음\" width=\"7\"/> 0\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</span>\n",
      "</li>\n",
      "<li class=\"rank09\">\n",
      "<a href=\"/webtoon/detail?titleId=748831&amp;no=92\" onclick=\"nclk_v2(event,'rnk*p.cont','748831','9')\" title=\"별을 삼킨 너에게-91화\">별을 삼킨 너에게-91화</a>\n",
      "<span class=\"rankBox\">\n",
      "<img alt=\"변동없음\" height=\"10\" src=\"https://ssl.pstatic.net/static/comic/images/migration/common/arrow_no.gif\" title=\"변동없음\" width=\"7\"/> 0\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</span>\n",
      "</li>\n",
      "<li class=\"rank10\">\n",
      "<a href=\"/webtoon/detail?titleId=758659&amp;no=36\" onclick=\"nclk_v2(event,'rnk*p.cont','758659','10')\" title=\"오빠세끼-#36 숨바꼭질\">오빠세끼-#36 숨바꼭질</a>\n",
      "<span class=\"rankBox\">\n",
      "<img alt=\"변동없음\" height=\"10\" src=\"https://ssl.pstatic.net/static/comic/images/migration/common/arrow_no.gif\" title=\"변동없음\" width=\"7\"/> 0\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t</span>\n",
      "</li>\n",
      "</ol>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://comic.naver.com/webtoon/weekday\"\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(res.text,\"lxml\")\n",
    "\n",
    "# 1위의 a태그 가져오기\n",
    "rank1 = soup.find(\"li\", attrs={\"class\":\"rank01\"})\n",
    "print(rank1.a.get_text())\n",
    "\n",
    "rank2 = soup.find(\"li\", attrs={\"class\":\"rank02\"})\n",
    "print(rank2.a.get_text())\n",
    "\n",
    "\n",
    "rank3 = soup.find(\"li\", attrs={\"class\":\"rank03\"})\n",
    "print(rank3.a.get_text())\n",
    "print('--'* 40)\n",
    "print(rank1.a.get_text())\n",
    "print(rank1.next_sibling.next_sibling.a.get_text())\n",
    "print(rank1.next_sibling.next_sibling.next_sibling.next_sibling.a.get_text())\n",
    "\n",
    "print('--'* 40)\n",
    "\n",
    "print(rank3.previous_sibling.previous_sibling.previous_sibling.previous_sibling.a.get_text())\n",
    "print(rank3.previous_sibling.previous_sibling.a.get_text())\n",
    "print(rank3.a.get_text())\n",
    "print('--'* 40)\n",
    "print(\"find: \",rank1.a.get_text())\n",
    "print(\"find: \",rank1.find_next_sibling(\"li\").a.get_text())\n",
    "print(\"find: \",rank1.find_next_sibling(\"li\").find_next_sibling(\"li\").a.get_text())\n",
    "\n",
    "ranks = rank1.find_next_siblings(\"li\")\n",
    "for rank in ranks:\n",
    "    print(rank.a.get_text())\n",
    "\n",
    "print('--'* 40)\n",
    "print(\"rank1.parent: \", rank1.parent)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "be49cb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 참교육\n",
      "2 : 신의 탑\n",
      "3 : 뷰티풀 군바리\n",
      "4 : 쇼미더럭키짱!\n",
      "5 : 윈드브레이커\n",
      "6 : 퀘스트지상주의\n",
      "7 : 소녀의 세계\n",
      "8 : 장씨세가 호위무사\n",
      "9 : 백수세끼\n",
      "10 : 팔이피플\n",
      "11 : 앵무살수\n",
      "12 : 잔불의 기사\n",
      "13 : 아마도\n",
      "14 : 만렙돌파\n",
      "15 : 똑 닮은 딸\n",
      "16 : 리턴 투 플레이어\n",
      "17 : 요리GO\n",
      "18 : 더블클릭\n",
      "19 : 파운더\n",
      "20 : 히어로메이커\n",
      "21 : 미니어처 생활백서\n",
      "22 : 평범한 8반\n",
      "23 : 칼가는 소녀\n",
      "24 : 유미의 작가 수칙\n",
      "25 : 결혼생활 그림일기\n",
      "26 : 물어보는 사이\n",
      "27 : 매지컬 급식\n",
      "28 : 아는 여자애\n",
      "29 : 순정말고 순종\n",
      "30 : 꼬리잡기\n",
      "31 : 황제와의 하룻밤\n",
      "32 : 결혼공략\n",
      "33 : 신군\n",
      "34 : 파견체\n",
      "35 : 꿈의 기업\n",
      "36 : 이별 후 사내 결혼\n",
      "37 : 이제야 연애\n",
      "38 : 입술이 예쁜 남자\n",
      "39 : 아, 쫌 참으세요 영주님!\n",
      "40 : 오빠집이 비어서\n",
      "41 : 제왕: 빛과 그림자\n",
      "42 : 또다시, 계약 부부\n",
      "43 : 싸이코 리벤지\n",
      "44 : 말박왕\n",
      "45 : 야생천사 보호구역\n",
      "46 : 홍천기\n",
      "47 : 바퀴\n",
      "48 : 사랑의 헌옷수거함\n",
      "49 : 원작은 완결난 지 한참 됐습니다만\n",
      "50 : 디나운스\n",
      "51 : 레지나레나 - 용서받지 못한 그대에게\n",
      "52 : 원하는 건 너 하나\n",
      "53 : 왕따협상\n",
      "54 : 최후의 금빛아이\n",
      "55 : 달로 만든 아이\n",
      "56 : 찌질하지만 로맨스는 하고 싶어\n",
      "57 : 모스크바의 여명\n",
      "58 : 나만의 고막남친\n",
      "59 : 별을 쫓는 소년들\n",
      "60 : 마지막 지수\n",
      "61 : 그림자 신부\n",
      "62 : 모락모락 왕세자님\n",
      "63 : 중독연구소\n",
      "64 : 개밥 먹는 남자\n",
      "65 : 악녀 18세 공략기\n",
      "66 : 역주행!\n",
      "67 : 사막에 핀 달\n",
      "68 : 헬로맨스\n",
      "69 : 남주서치\n",
      "70 : 오로지 오로라\n",
      "71 : 기사님을 지켜줘\n",
      "72 : 슈퍼스타 천대리\n",
      "73 : 결백한 사람은 없다\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://comic.naver.com/webtoon/weekday\"\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(res.text,\"lxml\")\n",
    "\n",
    "#월요일 웹툰 목록 가져오기\n",
    "day = soup.find(\"h4\", attrs={\"class\":\"mon\"})\n",
    "toons = day.find_next_sibling(\"ul\")\n",
    "cartoons = toons.find_all('li')\n",
    "\n",
    "for i, cartoon in enumerate(cartoons):\n",
    "    title = cartoon.find(\"a\", attrs={\"class\": \"title\"})\n",
    "    print(i+1,':', title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627da926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ca1db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b666f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870455b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51cbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0575d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e57f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62428577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
